#include "op-attrs/ops/broadcast.h"
#include "op-attrs/num_tensor_dims_t.h"
#include "op-attrs/tensor_dims.h"
#include "utils/exception.h"
#include "utils/record_formatter.h"

namespace FlexFlow {

RecordFormatter as_dot(BroadcastAttrs const &attrs) {
  RecordFormatter r;

  auto kv = [](std::string const &label, auto const &val) {
    RecordFormatter rr;
    rr << label << fmt::to_string(val);
    return rr;
  };

  for (ff_dim_t dim_idx : tensor_dims_range(get_num_dims(attrs.target_dims))) {
    r << kv(fmt::format("target_dims[{}]", dim_idx.value),
            dim_at_idx(attrs.target_dims, dim_idx));
  }

  return r;
}

tl::expected<TensorShape, std::string>
    get_output_shape(BroadcastAttrs const &attrs,
                     TensorShape const &input_shape) {
  if (get_num_dims(attrs.target_dims) < get_num_dims(input_shape.dims)) {
    return tl::unexpected(fmt::format(
        "get_output_shape for Broadcast expected num_dims(input_dims) <= "
        "num_dims(target_dims), but recieved input_shape {} with num dims "
        "greater than target_dims {}",
        input_shape,
        attrs.target_dims));
  }

  if (tensor_dims_is_broadcastable_to(input_shape.dims, attrs.target_dims)) {
    return TensorShape{attrs.target_dims, input_shape.data_type};
  } else {
    return tl::unexpected(fmt::format(
        "Input tensor shape {} is not broadcastable to target dims {}",
        input_shape,
        attrs.target_dims));
  }
}

ParallelTensorShape get_output_shape(BroadcastAttrs const &,
                                     ParallelTensorShape const &) {
  NOT_IMPLEMENTED();
}

} // namespace FlexFlow
